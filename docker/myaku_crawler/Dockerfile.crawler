# Builds a prod or dev image for the Myaku crawler.

FROM friedrice2/ubuntu.cron:1.0.8_18.04 AS base

# Set up deadsnakes ppa for installing newer Python versions
RUN apt-get update \
    && apt-get install -y software-properties-common \
    && add-apt-repository ppa:deadsnakes/ppa \
    && apt-get update

# Break python packages install into two layers so that the first install layer
# can be shared with images that only install those packages.
RUN apt-get install -y python3.7 python3-pip
RUN apt-get install -y python3.7-dev python3.7-gdbm

# Set python binary that should be used in the container
ENV PYTHON_BIN python3.7

ENV CRAWLER_BASE_DIR /crawler
ENV CRAWLER_RESOURCE_DIR $CRAWLER_BASE_DIR/resources
WORKDIR $CRAWLER_RESOURCE_DIR

# All needed to install ipadic-NEologd in the next run statement
RUN apt-get update && apt-get install -y build-essential curl git sudo file \
    wget mecab libmecab-dev mecab-ipadic-utf8

# Install ipadic-NEologd for use with MeCab (this can take several minutes)
RUN git clone https://github.com/neologd/mecab-ipadic-neologd.git --progress \
    && ./mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n -y

# Get latest JMdict
RUN wget http://ftp.monash.edu/pub/nihongo/JMdict_e.gz \
    && gunzip JMdict_e.gz \
    && mv JMdict_e JMdict_e.xml

ENV CRAWLER_SRC_DIR $CRAWLER_BASE_DIR/src

COPY ./LICENSE $CRAWLER_SRC_DIR/

COPY ./myaku/requirements.txt $CRAWLER_SRC_DIR/
RUN $PYTHON_BIN -m pip install -U pip \
    && $PYTHON_BIN -m pip install -r $CRAWLER_SRC_DIR/requirements.txt

ENV PYTHONPATH $PYTHONPATH:$CRAWLER_SRC_DIR
ENV MYAKU_SRC_DIR $CRAWLER_SRC_DIR/myaku
ENV SCRIPTS_DIR $CRAWLER_SRC_DIR/scripts

ENV IPADIC_NEOLOGD_GIT_DIR $CRAWLER_RESOURCE_DIR/mecab-ipadic-neologd
ENV JMDICT_XML_FILEPATH $CRAWLER_RESOURCE_DIR/JMdict_e.xml

ENV MYAKU_LOG_DIR $CRAWLER_BASE_DIR/log
VOLUME ["$MYAKU_LOG_DIR"]

ENV MYAKU_APP_DATA_DIR $CRAWLER_RESOURCE_DIR

ENV CRAWL_START_SCRIPT $SCRIPTS_DIR/start_crawl.sh
ENV CRAWL_PYTHON_SCRIPT $MYAKU_SRC_DIR/runners/run_crawl.py

# Can be modified at run time of the container and will still take effect.
ENV CRAWL_CRON_SCHEDULE "0 */8 * * *"

# Comma-separated list of the crawlers to use. Can be modified at run time of
# the container and will still take effect.
ENV CRAWLER_LIST "NhkNewsWeb"

# Intentionally insert the env variable name and not its value into the cron
# file so that the cron schedule can be swapped in for it at run time.
RUN echo "CRAWL_CRON_SCHEDULE root" \
    "flock -n /tmp/crawl.lockfile $CRAWL_START_SCRIPT" >> $CRON_FILE

WORKDIR $CRAWLER_SRC_DIR


FROM base AS prod

# Copy only the myaku module files needed to build the JMdict shelf.
COPY ./myaku/__init__.py $MYAKU_SRC_DIR/__init__.py
COPY ./myaku/_version.py $MYAKU_SRC_DIR/_version.py
COPY ./myaku/utils/__init__.py $MYAKU_SRC_DIR/utils/__init__.py
COPY ./myaku/japanese_analysis.py $MYAKU_SRC_DIR/japanese_analysis.py
COPY ./myaku/errors.py $MYAKU_SRC_DIR/errors.py
COPY ./myaku/datatypes.py $MYAKU_SRC_DIR/datatypes.py
COPY ./myaku/runners/build_jmdict_shelf.py \
    $MYAKU_SRC_DIR/runners/build_jmdict_shelf.py

RUN $PYTHON_BIN ./myaku/runners/build_jmdict_shelf.py

# Copy full source into image
COPY ./myaku $MYAKU_SRC_DIR
COPY ./docker/myaku_crawler/start_crawl.sh $CRAWL_START_SCRIPT

RUN chmod +x $CRAWL_START_SCRIPT


FROM base AS dev

# Install dev requirements like ipython only in the dev images so that the prod
# images don't contain them.
COPY ./requirements.txt $CRAWLER_SRC_DIR/dev_requirements.txt
RUN $PYTHON_BIN -m pip install -U pip \
    && $PYTHON_BIN -m pip install -r $CRAWLER_SRC_DIR/dev_requirements.txt

# Volumes for sharing in development source on host with container.
VOLUME ["$MYAKU_SRC_DIR"]
VOLUME ["$SCRIPTS_DIR"]

# Volume for persisting data only used in development such as ipython history.
ENV MYAKU_DEV_DATA_DIR $CRAWLER_BASE_DIR/devdata
ENV IPYTHONDIR $MYAKU_DEV_DATA_DIR/ipython
VOLUME ["$MYAKU_DEV_DATA_DIR"]
