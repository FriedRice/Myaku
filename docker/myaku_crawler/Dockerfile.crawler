# Builds a prod or dev image for the Myaku crawler.

FROM friedrice2/ubuntu.cron:1.0.1_18.04 AS base

ENV MYAKU_BASE_DIR /myaku
ENV MYAKU_RESOURCE_DIR $MYAKU_BASE_DIR/resources
WORKDIR $MYAKU_RESOURCE_DIR

# All needed to install ipadic-NEologd in the next run statement
RUN apt-get update && apt-get install -y build-essential curl git sudo file \
    mecab libmecab-dev mecab-ipadic-utf8

# Install ipadic-NEologd for use with MeCab (this can take several minutes)
RUN git clone https://github.com/neologd/mecab-ipadic-neologd.git --progress \
    && ./mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n -y

# Set up deadsnakes ppa for installing newer Python versions
RUN apt-get install -y software-properties-common \
    && add-apt-repository ppa:deadsnakes/ppa \
    && apt-get update

# Install all other needed packages
RUN apt-get install -y firefox wget python3.7 python3.7-dev python3-pip \
    python3-gdbm

# Get latest JMdict
RUN wget ftp://ftp.monash.edu.au/pub/nihongo/JMdict_e.gz \
    && gunzip JMdict_e.gz \
    && mv JMdict_e JMdict_e.xml

# Install geckodriver for use with Selenium
RUN wget https://github.com/mozilla/geckodriver/releases/download/v0.24.0\
/geckodriver-v0.24.0-linux64.tar.gz \
    && tar -xvzf geckodriver-v0.24.0-linux64.tar.gz \
    && chmod +x geckodriver \
    && mv geckodriver /usr/local/bin/ \
    && rm geckodriver-v0.24.0-linux64.tar.gz

# Set python binary that should be used in the container
ENV PYTHON_BIN python3.7

# Get latest pip
RUN $PYTHON_BIN -m pip install -U pip

ENV MYAKU_SRC_DIR $MYAKU_BASE_DIR/src
COPY ./requirements.txt $MYAKU_SRC_DIR/
RUN $PYTHON_BIN -m pip install -r $MYAKU_SRC_DIR/requirements.txt

ENV PYTHONPATH $PYTHONPATH:$MYAKU_SRC_DIR

ENV IPADIC_NEOLOGD_GIT_DIR $MYAKU_RESOURCE_DIR/mecab-ipadic-neologd
ENV JMDICT_XML_FILEPATH $MYAKU_RESOURCE_DIR/JMdict_e.xml

ENV MYAKU_LOG_DIR $MYAKU_BASE_DIR/log
VOLUME ["$MYAKU_LOG_DIR"]

ENV MYAKU_APP_DATA_DIR $MYAKU_BASE_DIR/appdata
VOLUME ["$MYAKU_APP_DATA_DIR"]

ENV CRAWL_START_SCRIPT $MYAKU_SRC_DIR/docker/myaku_crawler/start_crawl.sh
ENV CRAWL_PYTHON_SCRIPT $MYAKU_SRC_DIR/myaku/runners/run_crawl.py

# Can be modified at run time of the container and will still take effect.
ENV CRAWL_CRON_SCHEDULE "0 */8 * * *"

# Comma-separated list of the crawlers to use. Can be modified at run time of
# the container and will still take effect.
ENV CRAWLER_LIST "NhkNewsWeb"

# Intentionally insert the env variable name and not its value into the cron
# file so that the cron schedule can be swapped in for it at run time.
RUN echo "CRAWL_CRON_SCHEDULE root $CRAWL_START_SCRIPT" >> $CRON_FILE

WORKDIR $MYAKU_SRC_DIR


FROM base AS dev

# Volume for sharing in development source on host with container.
VOLUME ["$MYAKU_SRC_DIR"]

# Volume for persisting data only used in development such as ipython history.
ENV MYAKU_DEV_DATA_DIR $MYAKU_BASE_DIR/devdata
ENV IPYTHONDIR $MYAKU_DEV_DATA_DIR/ipython
VOLUME ["$MYAKU_DEV_DATA_DIR"]


FROM base AS prod

# Copy full source into image
COPY . $MYAKU_SRC_DIR

RUN chmod +x $CRAWL_START_SCRIPT
